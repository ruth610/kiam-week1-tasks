{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b136d1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "df = pd.read_csv('../data/raw_analyst_ratings')\n",
    "df.head()\n",
    "df.shape\n",
    "df.info()\n",
    "df.isna().sum()\n",
    "df.duplicated().sum(), df[df.duplicated(keep=False)].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae31f1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse dates robustly\n",
    "df['date'] = pd.to_datetime(df['date'], utc=True)  # interprets timezone offset and converts to UTC\n",
    "\n",
    "# If the date is given as local UTC-4 without offset string, add tz:\n",
    "# df['date'] = pd.to_datetime(df['date']).dt.tz_localize('Etc/GMT+4').dt.tz_convert('UTC')\n",
    "\n",
    "# Create extra columns\n",
    "df['date_utc'] = df['date']   # timezone-aware in UTC\n",
    "df['date_local'] = df['date_utc'].dt.tz_convert('America/New_York')  # example convert to US Eastern (if useful)\n",
    "df['date_only'] = df['date_utc'].dt.date\n",
    "df['hour_utc'] = df['date_utc'].dt.hour\n",
    "df['weekday'] = df['date_utc'].dt.day_name()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13dc7d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['headline_len_chars'] = df['headline'].str.len().fillna(0)\n",
    "df['headline_len_words'] = df['headline'].str.split().map(len).fillna(0)\n",
    "\n",
    "# Summary stats\n",
    "df[['headline_len_chars','headline_len_words']].describe()\n",
    "# Visualize distribution\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(df['headline_len_chars'], bins=50)\n",
    "plt.title(\"Headline length (chars)\")\n",
    "plt.xlabel(\"chars\"); plt.ylabel(\"count\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc9c3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count articles per publisher\n",
    "publisher_counts = df['publisher'].value_counts().reset_index()\n",
    "publisher_counts.columns = ['publisher','count']\n",
    "publisher_counts.head(20)\n",
    "\n",
    "# If publishers look like emails, extract domains\n",
    "import re\n",
    "def extract_domain(p):\n",
    "    if pd.isna(p): return None\n",
    "    m = re.search(r'@([A-Za-z0-9.-]+)', p)\n",
    "    if m: return m.group(1)\n",
    "    # try url-like\n",
    "    m2 = re.search(r'https?://([A-Za-z0-9.-]+)', p)\n",
    "    if m2: return m2.group(1)\n",
    "    # else try domain from publisher string (some sites include site.com in text)\n",
    "    return None\n",
    "\n",
    "df['publisher_domain'] = df['publisher'].apply(extract_domain)\n",
    "df['publisher_domain'].value_counts().head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe018140",
   "metadata": {},
   "outputs": [],
   "source": [
    "top = publisher_counts.head(15)\n",
    "plt.barh(top['publisher'][::-1], top['count'][::-1])\n",
    "plt.title(\"Top 15 publishers by article count\")\n",
    "plt.xlabel(\"article count\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d5ac14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# daily counts\n",
    "daily = df.set_index('date_utc').resample('D').size()\n",
    "daily.plot(figsize=(12,4), title=\"Daily article counts\")\n",
    "\n",
    "# hourly pattern across all days (use local hour if you converted)\n",
    "hourly = df['date_utc'].dt.hour.value_counts().sort_index()\n",
    "hourly.plot(kind='bar', title='Articles by hour (UTC)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18dae49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily.rolling(window=7).mean().plot(title=\"7-day rolling avg of daily articles\")\n",
    "daily.nlargest(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6aaf980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weekday activity\n",
    "weekday_counts = df['weekday'].value_counts().reindex(\n",
    "    [\"Monday\",\"Tuesday\",\"Wednesday\",\"Thursday\",\"Friday\",\"Saturday\",\"Sunday\"]\n",
    ")\n",
    "weekday_counts.plot(kind='bar', title=\"Articles by weekday\")\n",
    "\n",
    "# hour-of-day heatmap: hour vs weekday\n",
    "pivot = pd.crosstab(df['date_utc'].dt.hour, df['weekday'])\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.heatmap(pivot, cmap='viridis')\n",
    "plt.title(\"Heatmap of articles: hour vs weekday\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fc8216",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(s):\n",
    "    if pd.isna(s): return \"\"\n",
    "    s = s.lower()\n",
    "    s = re.sub(r\"[^a-z0-9\\s]\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "df['headline_clean'] = df['headline'].apply(clean_text)\n",
    "all_tokens = df['headline_clean'].str.split().explode()\n",
    "freq = all_tokens[~all_tokens.isin(stop)].value_counts()\n",
    "freq.head(30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65700318",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# bigrams\n",
    "vect = CountVectorizer(ngram_range=(2,2), min_df=10, stop_words='english')\n",
    "X = vect.fit_transform(df['headline_clean'].fillna(''))\n",
    "bigram_counts = pd.Series(X.sum(axis=0).A1, index=vect.get_feature_names_out()).sort_values(ascending=False)\n",
    "bigram_counts.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0750575e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "vect = CountVectorizer(max_df=0.95, min_df=10, stop_words='english')\n",
    "X = vect.fit_transform(df['headline_clean'].fillna(''))\n",
    "\n",
    "n_topics = 8\n",
    "lda = LatentDirichletAllocation(n_components=n_topics, random_state=0, learning_method='online')\n",
    "lda.fit(X)\n",
    "\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_features = [feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]\n",
    "        print(f\"Topic {topic_idx}: {', '.join(top_features)}\\n\")\n",
    "\n",
    "print_top_words(lda, vect.get_feature_names_out(), 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be819e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get topic per headline (dominant)\n",
    "topic_dist = lda.transform(X)  # shape (n_articles, n_topics)\n",
    "df['dominant_topic'] = topic_dist.argmax(axis=1)\n",
    "\n",
    "# Topic counts per stock\n",
    "topic_by_stock = pd.crosstab(df['stock'], df['dominant_topic']).apply(lambda r: r/r.sum(), axis=1)\n",
    "# topic_by_stock.head()\n",
    "\n",
    "# Publisher-topic mix\n",
    "topic_by_publisher = pd.crosstab(df['publisher'], df['dominant_topic'])\n",
    "topic_by_publisher_norm = topic_by_publisher.div(topic_by_publisher.sum(axis=1), axis=0)\n",
    "topic_by_publisher_norm.loc[top_publishers_list]  # inspect only top publishers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4a7c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['date_day'] = pd.to_datetime(df['date_utc'].dt.date)\n",
    "topic_ts = df.groupby(['date_day','dominant_topic']).size().unstack(fill_value=0)\n",
    "topic_ts.plot(subplots=True, figsize=(12, 2*n_topics), sharex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e631ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix for numeric features\n",
    "num = df[['headline_len_chars','headline_len_words']]\n",
    "num.corr()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce709fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['stock'] = df['stock'].str.upper().str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755ae9d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
